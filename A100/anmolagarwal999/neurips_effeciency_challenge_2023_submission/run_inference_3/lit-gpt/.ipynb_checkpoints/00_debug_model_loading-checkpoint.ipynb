{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edb6b4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-18 04:35:29.693041: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pprint\n",
    "import sys\n",
    "import os\n",
    "#os.environ[\"HUGGINGFACE_HUB_CACHE\"]=\"/submission/hf_cache\"\n",
    "import re, json, requests\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, BitsAndBytesConfig\n",
    "#####\n",
    "from transformers import LlamaForCausalLM, CodeLlamaTokenizer\n",
    "from transformers import LlamaForCausalLM\n",
    "import colorama\n",
    "colorama.init()\n",
    "\n",
    "####\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "# from human_eval.data import write_jsonl\n",
    "import time\n",
    "from optimum.bettertransformer import BetterTransformer\n",
    "from transformers import LlamaForCausalLM\n",
    "from transformers import LlamaTokenizer\n",
    "import datetime\n",
    "import pytz\n",
    "import subprocess\n",
    "import pynvml\n",
    "pynvml.nvmlInit()\n",
    "\n",
    "#########################################\n",
    "from fastapi import FastAPI\n",
    "\n",
    "import logging\n",
    "\n",
    "# Lit-GPT imports\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "from lit_gpt import GPT, Tokenizer, Config\n",
    "from lit_gpt.utils import lazy_load, quantization\n",
    "\n",
    "# Toy submission imports\n",
    "from helper import toysubmission_generate\n",
    "from api import (\n",
    "    ProcessRequest,\n",
    "    ProcessResponse,\n",
    "    TokenizeRequest,\n",
    "    TokenizeResponse,\n",
    "    Token,\n",
    "    DecodeRequest,\n",
    "    DecodeResponse\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be1e2b62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/anmol/.cache/huggingface/token\n",
      "Login successful\n",
      "Huggingface login done\n",
      "Starting time is: 1697628931.6823015\n",
      "Device is: cuda\n",
      "___________________________________________\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN version incompatibility: PyTorch was compiled  against (8, 9, 2) but found runtime version (8, 5, 0). PyTorch already comes bundled with cuDNN. One option to resolving this error is to ensure PyTorch can find the bundled cuDNN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_463046/3210701587.py\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"___________________________________________\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__CUDNN VERSION:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__Number CUDA Devices:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/wizard_coder/lib/python3.8/site-packages/torch/backends/cudnn/__init__.py\u001b[0m in \u001b[0;36mversion\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;34m\"\"\"Returns the version of cuDNN\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__cudnn_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/wizard_coder/lib/python3.8/site-packages/torch/backends/cudnn/__init__.py\u001b[0m in \u001b[0;36m_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m                         )\n\u001b[1;32m     69\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_error_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN version incompatibility: PyTorch was compiled  against (8, 9, 2) but found runtime version (8, 5, 0). PyTorch already comes bundled with cuDNN. One option to resolving this error is to ensure PyTorch can find the bundled cuDNN."
     ]
    }
   ],
   "source": [
    "\n",
    "##################\n",
    "login(token=\"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\")\n",
    "print(\"Huggingface login done\")\n",
    "# sys.exit(0)\n",
    "start = time.time()\n",
    "print(f\"Starting time is: {start}\")\n",
    "\n",
    "# print(\"Environment variables are: \", os.environ)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "assert(device=='cuda')\n",
    "\n",
    "try:\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "except:\n",
    "    pass\n",
    "print(f\"Device is: {device}\")\n",
    "\n",
    "if device==\"cuda\":\n",
    "    print(\"___________________________________________\")\n",
    "    print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "    print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "    for _i in range(torch.cuda.device_count()):\n",
    "        print(\"Stats for device: \", _i)\n",
    "        print('__CUDA Device Name:',torch.cuda.get_device_name(_i))\n",
    "        print('__CUDA Device Total Memory [GB]:',torch.cuda.get_device_properties(_i).total_memory/1e9)\n",
    "    print(\"___________________________________________\")\n",
    "\n",
    "######################################\n",
    "\n",
    "\n",
    "def print_time():\n",
    "    # Define the desired timezone using the GMT offset and minutes\n",
    "    desired_timezone = pytz.timezone('Asia/Calcutta')\n",
    "\n",
    "    # Get the current time in UTC\n",
    "    current_time_utc = datetime.datetime.utcnow()\n",
    "\n",
    "    # Convert the UTC time to the desired timezone\n",
    "    current_time_in_desired_timezone = current_time_utc.replace(tzinfo=pytz.utc).astimezone(desired_timezone)\n",
    "\n",
    "    # Format and print the time\n",
    "    formatted_time = current_time_in_desired_timezone.strftime('%Y-%m-%d %H:%M:%S %Z%z')\n",
    "    print(f\"Time in GMT+5.5 is: {formatted_time}\", flush=True)\n",
    "\n",
    "\n",
    "def print_gpu_memory_stats():\n",
    "    print(\"________________\")\n",
    "    def get_gpu_utilization(handle_now):\n",
    "        info = pynvml.nvmlDeviceGetUtilizationRates(handle_now)\n",
    "        return info.gpu\n",
    "    \n",
    "    def get_gpu_count():\n",
    "        try:\n",
    "            output = subprocess.check_output([\"nvidia-smi\", \"--query-gpu=index\", \"--format=csv,noheader\"])\n",
    "            gpu_count = len(output.strip().split(b\"\\n\"))\n",
    "            return gpu_count\n",
    "        except subprocess.CalledProcessError:\n",
    "            return 0\n",
    "    #############################################\n",
    "    num_gpus = get_gpu_count()\n",
    "    print_time()\n",
    "    print(\"Number of available GPUs:\", num_gpus)\n",
    "    handles = [pynvml.nvmlDeviceGetHandleByIndex(curr_gpu_id) for curr_gpu_id in range(num_gpus)]\n",
    "    TOT_MEM_CONSUMED = 0\n",
    "    for curr_gpu_id, handle in enumerate(handles):\n",
    "        info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "        memory_used = info.used / 1024 ** 2  # Convert bytes to megabytes\n",
    "        memory_total = info.total / 1024 ** 2  # Convert bytes to megabytes\n",
    "        # memory_free = memory_total - memory_used\n",
    "        frac_used = memory_used/memory_total\n",
    "        gpu_utilization = get_gpu_utilization(handle)\n",
    "        gpu_dict = dict()\n",
    "        gpu_dict = {\"curr_gpu\":curr_gpu_id,\n",
    "                    \"volatile_gpu_utils\": gpu_utilization,\n",
    "                    \"total_gpu_mem\":memory_total, \n",
    "                    \"used_gpu_mem\":memory_used, \n",
    "                    \"frac_used_gpu_mem\":frac_used\n",
    "        }\n",
    "        TOT_MEM_CONSUMED += memory_used\n",
    "        # curr_stats_dict[curr_gpu_id] = gpu_dict\n",
    "        # print(f\"Volatile GPU Utilization: {gpu_utilization}%\")\n",
    "        # print(f\"{datetime.datetime.now()} : Frac: {frac_used} | Used GPU Memory: {memory_used:.2f} MB | Free GPU Memory: {memory_free:.2f} MB | tOTAL IS: {memory_total}\")\n",
    "        # break\n",
    "        print(gpu_dict)\n",
    "    print(\"Total memory consumed is: \", TOT_MEM_CONSUMED)\n",
    "    print(\"________________\")\n",
    "    \n",
    "def fetch_llama_2_chat_prompt(initial_inst):\n",
    "    # links to reference: https://huggingface.co/blog/codellama#conversational-instructions , https://www.reddit.com/r/LocalLLaMA/comments/155po2p/comment/jsvn5md/?utm_source=share&utm_medium=web2x&context=3  , https://huggingface.co/blog/llama2#how-to-prompt-llama-2\n",
    "    prompt_now = lambda x:f'''<s>[INST] <<SYS>>\n",
    "    You are a helpful, respectful and honest teaching assistant for an introductory programming course in Matlab and C. Your current task is to answer student queries on Piazza. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "    If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "    <</SYS>>\n",
    "    \n",
    "{x} [/INST]'''\n",
    "    return prompt_now(initial_inst)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_model(\n",
    "    load_8bit: bool = False,\n",
    "    base_model: str = \"bigcode/starcoder\", \n",
    "    load_model_in_4_bit = False):\n",
    "    assert base_model, (\"Please specify a --base_model, e.g. --base_model='bigcode/starcoder'\"    )\n",
    "    print(\"Base model being used: \", base_model)\n",
    "\n",
    "    if \"meta-llama\" in base_model or \"anmolagarwal999\" in base_model or \"models_saved\" in base_model:\n",
    "        print(\"Model family is LLAMA-2\")\n",
    "        load_8bit = True if \"70b\" in base_model else False\n",
    "        try:\n",
    "            tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "        except:\n",
    "            print(\"Force loading tokenizer of another model\")\n",
    "            tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-hf\")\n",
    "        tokenizer.add_special_tokens(\n",
    "            {\n",
    "                \n",
    "                \"pad_token\": \"<PAD>\",\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(\"Tokenizer has been loaded.\")\n",
    "        model = LlamaForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        return_dict=True,\n",
    "        device_map=\"auto\", \n",
    "        load_in_8bit=load_8bit\n",
    "            )\n",
    "        #NOTE: Added tokenizer padding preference to LEFT (special padding side documentation for LLAMA-2: https://github.com/huggingface/transformers/issues/25022#issuecomment-1647573640 )\n",
    "        # TODO: follow up issue here: https://github.com/huggingface/transformers/issues/26072\n",
    "        assert(tokenizer.padding_side == 'right')\n",
    "        model.resize_token_embeddings(model.config.vocab_size + 1) \n",
    "        print(\"Value of load_8bit is: \", load_8bit)\n",
    "        \n",
    "    elif \"codellama\" in base_model:\n",
    "        print(\"Model family is CodeLLAMA\")\n",
    "        other_args = dict()\n",
    "        #########\n",
    "        # tokenizer = CodeLlamaTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\")\n",
    "        # model = LlamaForCausalLM.from_pretrained(\"codellama/CodeLlama-7b-hf\")\n",
    "        \n",
    "        ########\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "        \n",
    "        ###############################################\n",
    "        tokenizer.add_special_tokens(\n",
    "        {\n",
    "         \n",
    "            \"pad_token\": \"<PAD>\",\n",
    "        }\n",
    "    )\n",
    "        \n",
    "        \n",
    "        ############################################\n",
    "        print(\"Tokenizer has been loaded.\")\n",
    "        \n",
    "        if \"34b\" in base_model and  load_model_in_4_bit:\n",
    "            print(\"NOTE: Warning: LOADING MODEL IN 4BIT\")\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16\n",
    "            )\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                base_model,\n",
    "                device_map=\"auto\",\n",
    "                quantization_config=quantization_config\n",
    "            )\n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                base_model,\n",
    "                device_map=\"auto\",\n",
    "                torch_dtype=torch.float16\n",
    "            )\n",
    "        #TODO: Know more about the padding\n",
    "        model.resize_token_embeddings(model.config.vocab_size + 1) \n",
    "        tokenizer.padding_side = 'right'\n",
    "        # assert(tokenizer.padding_side == 'right')\n",
    "\n",
    "    else:\n",
    "        print(\"Model family is WizardCoder\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "        # new addition\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(\"Tokenizer has been loaded.\")\n",
    "        \n",
    "        if device == \"cuda\":\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                base_model,\n",
    "                load_in_8bit=load_8bit,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "            )\n",
    "        elif device == \"mps\":\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                base_model,\n",
    "                device_map={\"\": device},\n",
    "                torch_dtype=torch.float16,\n",
    "            )\n",
    "        #NOTE: Added tokenizer padding preference to LEFT\n",
    "        tokenizer.padding_side = 'left'\n",
    "            \n",
    "    ### Memory stats before BetterTransformer\n",
    "    print(\"Memory stats before BetterTransformer\")\n",
    "    print_gpu_memory_stats()\n",
    "    \n",
    "    \n",
    "    # adding BetterTransformer functionality\n",
    "    # new_model = BetterTransformer.transform(model, keep_original_model=True if os.environ.get(\"AMLT_DIRSYNC_DIR\", None) is None else False)\n",
    "    new_model = BetterTransformer.transform(model, keep_original_model=False)\n",
    "    del model\n",
    "    print(\"Memory stats after deletion\")\n",
    "    print_gpu_memory_stats()\n",
    "    model = new_model\n",
    "    \n",
    "\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    print(\"Config pad token id is: \", model.config.pad_token_id )\n",
    "\n",
    "    try:\n",
    "        if not load_8bit:\n",
    "            model.half()  # seems to fix bugs for some users.\n",
    "    except Exception as E:\n",
    "        print(\"ERROR: Some error occurred during model.half(): \", E)\n",
    "\n",
    "    model.eval()\n",
    "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    print(\"Model distribution is: \", model.hf_device_map, flush=True)\n",
    "    \n",
    "    ####################################\n",
    "    ### Memory stats after BetterTransformer\n",
    "    print(\"Memory stats AFTER BetterTransformer\")\n",
    "    print_gpu_memory_stats()\n",
    "    \n",
    "    \n",
    "    ######################################\n",
    "    print(\"TOKENIZER stats are: \", tokenizer.__dict__, \"\\n#########\\n\")\n",
    "    \n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83397ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"/home/anmol/nips_challenge/efficiency_challenge_repo/code/00_starter_repo/neurips_llm_efficiency_challenge/sample-submissions/llama_recipes/models_saved/32_32_8fa6081f-d6df-4d0b-95d1-f209962c35b1/WHOLE_best_model_yet_epoch_8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5d7a18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e464b98a52394965ab99bda6bf6b86f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "        return_dict=True,\n",
    "        device_map=\"auto\", \n",
    "        load_in_8bit=False\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f540758",
   "metadata": {},
   "outputs": [],
   "source": [
    "!whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4838ebfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "        ,\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wizard_coder_kernel",
   "language": "python",
   "name": "wizard_coder_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
