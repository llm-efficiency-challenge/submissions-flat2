{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge all datasets together and reduce dataset based on similarity metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sentence-transformers tqdm ipywidgets numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib as plb\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data/processed/\"\n",
    "EMBEDDING_DIR = \"../../data/instruction_embeddings\"\n",
    "data_dir = plb.Path(DATA_DIR)\n",
    "embedding_dir = plb.Path(EMBEDDING_DIR)\n",
    "\n",
    "embedding_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = data_dir.glob(\"*.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for f in files:\n",
    "    if f.stem == \"helm_training_data\":\n",
    "        continue\n",
    "    df = pd.read_json(f, lines=True)\n",
    "    if not 'dataset' in df.columns:\n",
    "        df[\"dataset\"] = f.stem\n",
    "    data.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.concat(data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data[\"dataset\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_bias = df_data.loc[lambda df: df['dataset'].isin(['stereoset_intersentence', 'stereoset_intrasentence', 'crows_pairs'])].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_data.loc[lambda df: ~df['dataset'].isin(['stereoset_intersentence', 'stereoset_intrasentence', 'crows_pairs'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_helm = pd.read_json(data_dir / \"helm_training_data.jsonl\", lines=True).drop(columns=[\"method\"], axis=1)\n",
    "df_helm.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter data using cosine similarity\n",
    "\n",
    "See [open platypus](https://github.com/arielnlee/Platypus/blob/main/data_pipeline/data_similarity.ipynb) repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data[\"combined_instruction\"] = (df_data[\"instruction\"] + ' ' + df_data[\"input\"]).str.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data[\"combined_instruction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\"\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_instructions = df_data[\"combined_instruction\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_embeddings = model.encode(concatenated_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(EMBEDDING_DIR + \"/instruction-MiniLM-L6-v2-embeddings\", instruction_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_embeddings = np.load(EMBEDDING_DIR + \"/instruction-MiniLM-L6-v2-embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle rows\n",
    "df_data_shuffled = df_data.sample(frac=1, random_state=3724376)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = df_data_shuffled.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def cosim(u, M):\n",
    "    scores = np.zeros(M.shape[0])\n",
    "    for i in numba.prange(M.shape[0]):\n",
    "        v = M[i]\n",
    "        m = u.shape[0]\n",
    "        udotv = 0\n",
    "        u_norm = 0\n",
    "        v_norm = 0\n",
    "        for j in range(m):\n",
    "            if (np.isnan(u[j])) or (np.isnan(v[j])):\n",
    "                continue\n",
    "\n",
    "            udotv += u[j] * v[j]\n",
    "            u_norm += u[j] * u[j]\n",
    "            v_norm += v[j] * v[j]\n",
    "\n",
    "        u_norm = np.sqrt(u_norm)\n",
    "        v_norm = np.sqrt(v_norm)\n",
    "\n",
    "        if (u_norm == 0) or (v_norm == 0):\n",
    "            ratio = 1.0\n",
    "        else:\n",
    "            ratio = udotv / (u_norm * v_norm)\n",
    "        scores[i] = ratio\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosim(instruction_embeddings[0], instruction_embeddings[:10000,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_embeddings = np.empty((0, 384), np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate((existing_embeddings, instruction_embeddings[0,:].reshape(1,-1)), axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = []\n",
    "existing_embeddings = np.empty((0, 384), np.float32)\n",
    "\n",
    "for r in tqdm.notebook.tqdm(enumerate(json_data)):\n",
    "    i, d = r\n",
    "    if not final_data:\n",
    "        final_data.append(d)\n",
    "        existing_embeddings = np.concatenate((existing_embeddings, instruction_embeddings[i].reshape(1, -1)), axis=0)\n",
    "    else:\n",
    "        similarity_scores = cosim(instruction_embeddings[i], existing_embeddings)\n",
    "        \n",
    "        if np.max(similarity_scores) <= 0.75:\n",
    "            final_data.append(d)\n",
    "            existing_embeddings = np.concatenate((existing_embeddings, instruction_embeddings[i].reshape(1, -1)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dedup = pd.DataFrame(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dedup.to_json(data_dir.parent / \"sft_train_shuffled_reduced_all_ds_nshot.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dedup = pd.read_json(data_dir.parent / \"sft_train_shuffled_reduced_all_ds_nshot.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_medq = df_dedup.loc[lambda df: df[\"dataset\"] == \"MedQuad\"].sample(1000, random_state=674675)\n",
    "df_dedup = df_dedup.loc[lambda df: df[\"dataset\"] != \"MedQuad\"]\n",
    "\n",
    "df_dedup = pd.concat([df_dedup, df_medq], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_dedup, df_helm, df_data_bias], ignore_index=True).sample(frac=1, random_state=123663).reset_index(drop=True).drop(columns=[\"combined_instruction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_json(data_dir.parent / \"sft_train_shuffled_reduced_all_ds.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dedup['dataset'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dedup.to_json('../../data/sft_train_shuffled_reduced_all_ds.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
