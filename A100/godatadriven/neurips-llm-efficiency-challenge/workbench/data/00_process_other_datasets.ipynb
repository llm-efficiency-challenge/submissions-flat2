{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "d = load_dataset('Amirkid/MedQuad-dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['train'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = []\n",
    "a = []\n",
    "\n",
    "for i in range(len(d['train'])):\n",
    "    if i % 2 == 0:\n",
    "        q.append(d['train'][i]['text'])\n",
    "    else:\n",
    "        aq = d['train'][i]['text']\n",
    "        if aq.startswith('Summary : '):\n",
    "            aq = aq.replace('Summary : ', '')\n",
    "        a.append(aq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'instruction': q,\n",
    "    'input': '',\n",
    "    'output': a,\n",
    "    'dataset': 'MedQuad'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_medquad = df.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SciQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from cajajejo.commands.api.utils import MAP\n",
    "\n",
    "def fmt_output_mc(x):\n",
    "    return x['correct']\n",
    "\n",
    "def fmt_q(x):\n",
    "    if x['method'] == 'multiple_choice_joint':\n",
    "        references = '\\n'.join([f\"{MAP[i]}. {r.strip()}\" for i, r in enumerate(x['references'].split(';'))])\n",
    "        prompt = f\"\"\"{x['prefix']}{x['task']}\\n{references}\\n{x['output_prefix'].strip()} {fmt_output_mc(x)}\"\"\".strip()\n",
    "    else:\n",
    "        references = x['references']\n",
    "        prompt = f\"\"\"{x['prefix']}{x['task']}\\n{x['output_prefix']}{references}\"\"\".strip()\n",
    "    return prompt\n",
    "\n",
    "def n_shot_examples(df, random_state: int, n_examples: int = 2, n_shot_postfix = \"\\n\\n\"):\n",
    "    n_examples_req_per_sample = n_examples + 1\n",
    "    d_n = df.shape[0]\n",
    "    ns_s = d_n // n_examples_req_per_sample\n",
    "    if d_n - ns_s < 0:\n",
    "        raise ValueError(\"Not enough samples\")\n",
    "    print(f\"Total input samples: {d_n}\")\n",
    "    print(f\"Examples required per N-shot sample: {n_examples_req_per_sample}\")\n",
    "    print(f\"N-shot samples: {ns_s}\")\n",
    "    dshuf = df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    dshuf[\"instruction_fmt\"] = dshuf.apply(fmt_q, axis=1)\n",
    "    instruction = dshuf[\"instruction\"].iloc[0]\n",
    "    samples_ns = np.array_split(dshuf, ns_s)\n",
    "    n_shot_questions = []\n",
    "    n_shot_answers = []\n",
    "    for s in samples_ns:\n",
    "        if s.shape[0] < n_examples_req_per_sample:\n",
    "            continue\n",
    "        sl = s.iloc[-1]\n",
    "        len_answer = len(sl['correct']) if sl['method'] == 'multiple_choice_joint' else len(sl['references'])\n",
    "        comb = (instruction + \"\\n\" + f\"{n_shot_postfix}\".join(s[\"instruction_fmt\"])).strip()\n",
    "        nsh_q = comb[:(len(comb) - len_answer)].strip()\n",
    "        nsh_a = comb[-len_answer:].strip()\n",
    "        n_shot_questions.append(nsh_q)\n",
    "        n_shot_answers.append(nsh_a)\n",
    "    dfr = pd.DataFrame(\n",
    "        {\n",
    "            \"question\": n_shot_questions,\n",
    "            \"answer\": n_shot_answers,\n",
    "            \"method\": sl['method'],\n",
    "        }\n",
    "    )\n",
    "    return dfr\n",
    "\n",
    "\n",
    "def fmt_mc(x):\n",
    "    answers = x.loc[[\"correct_answer\", \"distractor1\", \"distractor2\", \"distractor3\"]].values.tolist()\n",
    "    correct, _, _, _ = answers\n",
    "\n",
    "    np.random.shuffle(answers)\n",
    "\n",
    "    answers_fmt = []\n",
    "\n",
    "    for i, ans in enumerate(answers):\n",
    "        if ans == correct:\n",
    "            correct_ident = MAP[i]\n",
    "        opt = ans\n",
    "        answers_fmt.append(opt)\n",
    "    \n",
    "    return correct_ident, \";\".join(answers_fmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sh\n",
    "\n",
    "link = 'https://ai2-public-datasets.s3.amazonaws.com/sciq/SciQ.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh.curl(link, '-o', 'SciQ.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh.unzip('SciQ.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib as plb\n",
    "\n",
    "unz_data = [*plb.Path('.').glob('SciQ dataset*')][0].resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "splits = []\n",
    "\n",
    "for split in [\"train.json\", \"valid.json\", \"test.json\"]:\n",
    "    df = pd.read_json(unz_data / split, lines=False)\n",
    "    df['split'] = split\n",
    "    splits.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(splits, axis=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(78463)\n",
    "\n",
    "mcq = df.apply(fmt_mc, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = [a[0] for a in mcq]\n",
    "ref = [a[1] for a in mcq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"Question: \"\n",
    "output_prefix = \"Answer:\"\n",
    "method = \"multiple_choice_joint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'task': df['question'].tolist(),\n",
    "    'instruction': '',\n",
    "    'prefix': prefix,\n",
    "    'output_prefix': output_prefix,\n",
    "    'method': method,\n",
    "    'references': ref,\n",
    "    'correct': correct\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, random_state=21853).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_shot = n_shot_examples(df.loc[:5000], random_state=24122, n_examples=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_five_shot = n_shot_examples(df.loc[5001:], random_state=98864, n_examples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_one_shot, df_five_shot], axis=0)\n",
    "\n",
    "df[\"dataset\"] = \"SciQ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename({\"question\": \"instruction\", \"answer\": \"output\"}, axis=1, inplace=False)\n",
    "df[\"input\"] = \"Choose the best option out of the choices given, and return the letter corresponding to the option you choose.\"\n",
    "df = df.drop([\"method\"], axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sciq = df.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle challenge\n",
    "\n",
    "from https://www.kaggle.com/competitions/kaggle-llm-science-exam/data?select=test.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = '/Users/user/Documents/CodeProjects/neurips-llm-efficiency-challenge-2023/neurips-llm-efficiency-challenge/data/kaggle/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmt(x):\n",
    "    answers = [\n",
    "        f\"A. {x['A']}\",\n",
    "        f\"B. {x['B']}\",\n",
    "        f\"C. {x['C']}\",\n",
    "        f\"D. {x['D']}\"\n",
    "        f\"E. {x['E']}\"\n",
    "    ]\n",
    "    answers_str = \"\\n\".join(answers)\n",
    "    p = f\"\"\"\n",
    "Question: {x['prompt']}\n",
    "{answers_str}\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "    return {\n",
    "        'instruction': p,\n",
    "        'input': \"\",\n",
    "        'output': x[\"answer\"]\n",
    "    }    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kag = pd.DataFrame(df_train.apply(fmt, axis=1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kag[\"dataset\"] = \"kaggle\"\n",
    "df_kag[\"input\"] = \"Choose the best option out of the choices given, and return the letter corresponding to the option you choose.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stereoset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "\n",
    "intra = datasets.load_dataset(\"stereoset\", 'intrasentence') #'intersentence')\n",
    "inter = datasets.load_dataset(\"stereoset\", 'intersentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intra = intra['validation'].to_pandas()\n",
    "df_inter = inter['validation'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.where(df_inter.iloc[0].sentences['gold_label'] == 0)[0][0]\n",
    "\n",
    "df_inter.iloc[0].sentences['sentence'][p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "LABEL_MAP = {0: 'anti-stereotype', 1: 'stereotype', 2: 'unrelated'}\n",
    "\n",
    "instruction = \"Rewrite the passage to remove the stereotype.\"\n",
    "instruction_postfix = \"\\n\\n\"\n",
    "passage_prefix = \"Passage: \"\n",
    "passage_postfix = \"\\n\\n\"\n",
    "rewritten_prefix = \"Answer: \"\n",
    "\n",
    "def get_label(x, label_idx: int):\n",
    "    sentence_pos = np.where(x['gold_label'] == label_idx)[0][0]\n",
    "    return x['sentence'][sentence_pos]\n",
    "    \n",
    "\n",
    "df_inter['anti_stereotype'] = df_inter['sentences'].apply(get_label, label_idx=0)\n",
    "df_inter['stereotype'] = df_inter['sentences'].apply(get_label, label_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inter['passage'] = df_inter['context'].str.rstrip(\".\") + \". \" + df_inter['stereotype']\n",
    "df_inter['output'] = df_inter['context'].str.rstrip(\".\") + \". \" + df_inter['anti_stereotype']\n",
    "\n",
    "df_inter.iloc[0].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmt_prompt(x):\n",
    "    return f\"\"\"\n",
    "{instruction}{instruction_postfix}{passage_prefix}{x['passage']}{passage_postfix}{rewritten_prefix}\n",
    "    \"\"\".strip()\n",
    "\n",
    "df_inter['instruction'] = df_inter.apply(fmt_prompt, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_inter.iloc[0].instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inter['input'] = ''\n",
    "df_inter['dataset'] = 'stereoset_intersentence'\n",
    "\n",
    "df_inter = df_inter.loc[:, ['instruction', 'input', 'output', 'dataset']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = 'Fill in the blank with the appropriate answer.'\n",
    "\n",
    "def get_label(x, label_idx: int):\n",
    "    sentence_pos = np.where(x['gold_label'] == label_idx)[0][0]\n",
    "    return x['sentence'][sentence_pos]\n",
    "\n",
    "df_intra['anti_stereotype'] = df_intra['sentences'].apply(get_label, label_idx=0)\n",
    "df_intra['stereotype'] = df_intra['sentences'].apply(get_label, label_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7316463)\n",
    "\n",
    "def fmt(x):\n",
    "    co = x[\"context\"]\n",
    "    st = x[\"stereotype\"]\n",
    "    ast = x[\"anti_stereotype\"]\n",
    "    io = [st, ast]\n",
    "    np.random.shuffle(io)\n",
    "    opts = \"\\n\".join([f\"A. {io[0]}\", f\"B. {io[1]}\"])\n",
    "    correct_idx = np.where(np.array(io) == ast)[0][0]\n",
    "    correct_label = 'A' if correct_idx == 0 else 'B'\n",
    "    return f\"\"\"{instruction}\\n\\nPassage: {co}\\n{opts}\\n\\nAnswer: {correct_label}\"\"\"\n",
    "\n",
    "df_intra['instruction'] = df_intra.apply(lambda df: fmt(df), axis=1)\n",
    "df_intra['input'] = \"Choose the best option out of the choices given, and return the letter corresponding to the option you choose.\"\n",
    "df_intra['output'] = df_intra['instruction'].str[-1]\n",
    "df_intra['instruction'] = df_intra['instruction'].str[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intra = df_intra.loc[:, ['instruction', 'input', 'output']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intra[\"dataset\"] = \"stereoset_intrasentence\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crows-pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "prompt = pd.read_csv('https://raw.githubusercontent.com/nyu-mll/crows-pairs/master/data/prompts.csv').loc[:, ['prompt', 'source']]\n",
    "resp = pd.read_csv('https://raw.githubusercontent.com/nyu-mll/crows-pairs/master/data/crows_pairs_anonymized.csv')\n",
    "resp[\"prompt\"] = prompt[\"prompt\"]\n",
    "resp = resp.loc[lambda df: df['bias_type'] != \"religion\"]\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = resp.loc[:, [\"prompt\", \"sent_less\", \"sent_more\"]]\n",
    "resp[\"passage\"] = resp[\"prompt\"] + \" \" + resp[\"sent_more\"]\n",
    "resp[\"response\"] = resp[\"prompt\"] + \" \" + resp[\"sent_less\"]\n",
    "\n",
    "def fmt_prompt(x):\n",
    "    return f\"\"\"Rewrite the passage to remove the stereotype.\\n\\nPassage: {x['passage']}\\n\\nAnswer: \"\"\".strip()\n",
    "\n",
    "resp[\"instruction\"] = resp.apply(fmt_prompt, axis=1)\n",
    "\n",
    "resp[\"input\"] = \"\"\n",
    "\n",
    "resp.rename({\"response\": \"output\"}, axis=1, inplace=True)\n",
    "\n",
    "resp[\"dataset\"] = \"crows_pairs\"\n",
    "\n",
    "resp = resp.loc[:, [\"instruction\", \"input\", \"output\", \"dataset\"]]\n",
    "\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_other = pd.concat([df_medquad, df_sciq, df_kag, df_inter, df_intra, resp], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_other.to_json('../../data/processed/otherdata.jsonl', lines=True, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
