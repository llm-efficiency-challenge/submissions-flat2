{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5f2176e-ff67-4b57-b001-b03d49734685",
   "metadata": {
    "id": "e5f2176e-ff67-4b57-b001-b03d49734685"
   },
   "source": [
    "# Llama2 Neurips exploratory model using TRL\n",
    "\n",
    "In this notebook, I\n",
    "\n",
    "1. Train a Llama2 13B model on an A100 40GB using TRL & transformers\n",
    "1. Upload the model weights to the MLFlow tracking server\n",
    "1. Test & document various ways to optimize model training\n",
    "\n",
    "The Neurips 2023 competition limits participants in (1) the models that can be used, and (2) the data that can be used. Hence, to be successful in the competition we need to make sure that we:\n",
    "\n",
    "1. Can evaluate faster than our competitors on the HELM benchmark\n",
    "1. Squeeze every inch of performance out of the A100 GPU that we're using on GCP. This should be big enough to load the 7B and perhaps the 13B models dependending on the settings. There are a bunch of tricks for this, see [here](https://lightning.ai/pages/community/tutorial/pytorch-memory-vit-llm).\n",
    "1. Use as much data as we can find. This data needs to be open-source and cannot be machine-generated.\n",
    "\n",
    "[Lightning](https://lightning.ai/pages/community/tutorial/neurips2023-llm-efficiency-guide/) have made a nice tutorial / getting started guide that supports most models. A [pull request](https://github.com/Lightning-AI/lit-gpt/pull/412) integrating Lit-GPT and HELM has recently been merged so we can use this framework to evaluate our models.\n",
    "\n",
    "See [this repository](https://github.com/Lightning-AI/lit-gpt) for the starter code used in this notebook.\n",
    "\n",
    "### GCP\n",
    "\n",
    "on GCP Vertex workbench, ensure to select the option 'Python 3 (with IntelÂ® MKL)', else we cannot install pytorch nightly properly\n",
    "\n",
    "To enable monitoring of the GPU, first enable jupyter extensions in the menu to the far-left of the jupyterlab instance.\n",
    "\n",
    "Then, install the `jupyterlab-nvdashboard` extension. NB: you must restart the notebook server for the dashboard to show up.\n",
    "\n",
    "### Notes\n",
    "\n",
    "1. I noticed that there is a [flash attention](https://github.com/huggingface/trl@flash-attn-sft) branch. I tried it out and it slows down training by quite a bit, probably due to the fact that we need to pack sequences and we cannot pad them as we would normally do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeee9895-8770-4dbd-a16a-a7f4a2fe40b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes trl mlflow datasets\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gyfbNejJ3iqM",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "type(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb01fea-3b6c-4ffb-99f6-b0be0ff0a5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from peft import (\n",
    "    get_peft_config,\n",
    "    PeftModel,\n",
    "    PeftConfig,\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainerCallback,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b024b96c-bb98-470d-b62e-12632f856945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from the lit-gpt repository\n",
    "def generate_prompt(example):\n",
    "    \"\"\"Generates a standardized message to prompt the model with an instruction, optional input and a\n",
    "    'response' field.\"\"\"\n",
    "\n",
    "    if example[\"context\"]:\n",
    "        return (\n",
    "            \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            f\"### Instruction:\\n{example['instruction']}\\n\\n### Input:\\n{example['context']}\\n\\n### Response:\"\n",
    "        )\n",
    "    return (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dbb294-1bae-4e34-b369-8e1689fd21c6",
   "metadata": {
    "id": "33dbb294-1bae-4e34-b369-8e1689fd21c6"
   },
   "source": [
    "### Load data\n",
    "\n",
    "We use dolly-15K for now, and format it using the lit-gpt formatting style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3d4a7e-43f3-4f67-b6b3-8ac2d587f2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dolly = load_dataset(\"databricks/databricks-dolly-15k\")\n",
    "df_dolly = pd.DataFrame(df_dolly['train'])\n",
    "df_dolly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8320afee-a45d-46b1-a70f-2ceddfce34fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dolly[\"prompt\"] = df_dolly.apply(generate_prompt, axis=1)\n",
    "df_dolly[\"response\"] = df_dolly[\"response\"] + \"\\n### End\"\n",
    "df_dolly = df_dolly[[\"prompt\", \"response\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173d097c-b963-4a61-be33-c79e7609d38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_dolly.copy()\n",
    "df[\"text\"] = df[\"prompt\"] + df[\"response\"]\n",
    "df.drop(columns=[\"prompt\", \"response\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e726cf-334d-4e3e-929e-d84a343285cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df).train_test_split(test_size=0.05, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfe4c6d-c8ef-42b9-aa20-0e90f9dcca38",
   "metadata": {
    "id": "8dfe4c6d-c8ef-42b9-aa20-0e90f9dcca38"
   },
   "source": [
    "### Model & training options\n",
    "\n",
    "We define:\n",
    "\n",
    "1. BitsAndBytes configuration for loading the base model efficiently\n",
    "1. Training options for the SFTtrainer\n",
    "1. PEFT options (LoRA)\n",
    "\n",
    "I'm following some best practices that you can find [here](https://huggingface.co/docs/transformers/perf_train_gpu_one). I also take some settings from the [Open Platypus paper](https://arxiv.org/pdf/2308.07317.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaea137-de9d-4a5c-8bae-7c5b9863a4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model\n",
    "model_id = \"facebook/opt-350m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ffd3b0-025b-4e13-b6b9-8b28f90b2f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## BitsAndBytes\n",
    "load_in_4bit = True\n",
    "bnb_4bit_use_double_quant = True\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "bnb_4bit_compute_dtype = torch.bfloat16\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    bnb_4bit_use_double_quant=bnb_4bit_use_double_quant,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=bnb_4bit_compute_dtype\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdbf892-bc53-4028-80a2-e61b58af274b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PEFT\n",
    "# target_modules = [\n",
    "#     'gate_proj','down_proj', 'up_proj',\n",
    "#     'k_proj','lm_head', 'q_proj',\n",
    "#     'v_proj','o_proj'\n",
    "# ]\n",
    "target_modules = [\n",
    "    \"k_proj\", \"q_proj\", \"v_proj\"\n",
    "]\n",
    "r = 16\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.05\n",
    "bias = \"none\"\n",
    "task_type = \"CAUSAL_LM\"\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=bias,\n",
    "    target_modules = target_modules,\n",
    "    task_type=task_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeb8b7d-4be3-418e-9bb3-7e52852f8b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training\n",
    "# TODO: something with logging & logging artifacts\n",
    "base_dir = \"out\"\n",
    "\n",
    "save_strategy=\"steps\"\n",
    "save_steps=100\n",
    "save_total_limit=3\n",
    "\n",
    "num_train_epochs = 2\n",
    "evaluation_strategy = \"steps\"\n",
    "logging_strategy = \"steps\"\n",
    "eval_steps = 100\n",
    "logging_steps = 25\n",
    "\n",
    "per_device_train_batch_size = 4\n",
    "gradient_accumulation_steps = 16 # virtual batch size = 4 * 8 = 64. See https://huggingface.co/docs/transformers/perf_train_gpu_one#batch-size-choice\n",
    "gradient_checkpointing = True\n",
    "per_device_eval_batch_size = 2\n",
    "eval_accumulation_steps = 8\n",
    "#max_steps=50 # only debugging\n",
    "\n",
    "learning_rate = 4e-4\n",
    "lr_scheduler_type = \"cosine\"\n",
    "max_grad_norm = 0.3\n",
    "warmup_steps = 100\n",
    "optim = 'paged_adamw_8bit' # See: https://huggingface.co/docs/transformers/perf_train_gpu_one#8bit-adam\n",
    "\n",
    "dataloader_pin_memory = True\n",
    "dataloader_num_workers = 1\n",
    "\n",
    "tf32 = False # See: https://huggingface.co/docs/transformers/perf_train_gpu_one#tf32\n",
    "group_by_length = True\n",
    "\n",
    "# Error: -- FileNotFoundError: [Errno 2] No such file or directory: 'ldconfig'\n",
    "torch_compile = False # See: https://huggingface.co/docs/transformers/perf_train_gpu_one#using-torchcompile\n",
    "\n",
    "if tf32:\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=base_dir,\n",
    "    evaluation_strategy=evaluation_strategy,\n",
    "    eval_steps=eval_steps,\n",
    "    num_train_epochs = num_train_epochs,\n",
    "    logging_strategy = logging_strategy,\n",
    "    logging_steps=logging_steps,\n",
    "    save_strategy=save_strategy,\n",
    "    save_steps=save_steps,\n",
    "    save_total_limit=save_total_limit,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    eval_accumulation_steps=eval_accumulation_steps,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    learning_rate=learning_rate,\n",
    "    #bf16=True,\n",
    "    #max_steps=max_steps,\n",
    "    tf32=True if tf32 else False,\n",
    "    fp16=True if not tf32 else False,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    warmup_steps=warmup_steps,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    group_by_length=group_by_length,\n",
    "    torch_compile=torch_compile,\n",
    "    dataloader_pin_memory=dataloader_pin_memory,\n",
    "    dataloader_num_workers=dataloader_num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d642e38e-3a06-4439-b476-b14bf7748a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other stuff\n",
    "#MLFLOW_TRACKING_URI=\"localhost\"\n",
    "#MLFLOW_EXPERIMENT=\"jasper-train-testing\"\n",
    "#mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad168525-60ce-4870-9d3e-fe53d125d122",
   "metadata": {
    "id": "ad168525-60ce-4870-9d3e-fe53d125d122"
   },
   "source": [
    "### Loading & configuring the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c13483e-69db-4325-aca5-bc7d72cef325",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# When loading 1st time this will be slow\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97a5f98-0d34-4159-ac32-1db2615f20a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d940b74-0c02-46d3-83db-ef9e9bc9f2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c46392-ec96-400a-a0be-4f5033081e3e",
   "metadata": {
    "id": "44c46392-ec96-400a-a0be-4f5033081e3e"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R5R-B80f_2Yf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers.integrations import MLflowCallback\n",
    "\n",
    "os.environ[\"HF_MLFLOW_LOG_ARTIFACTS\"] = \"1\"\n",
    "os.environ[\"MLFLOW_EXPERIMENT_NAME\"] = \"test\"\n",
    "\n",
    "callbacks = [MLflowCallback()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b0406b-3074-4571-97c7-2440cb5f1b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset = dataset['test'],\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=2048,\n",
    "    args=training_args,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493cb8a1-837d-4214-9364-8a398e2884a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ec7d24-f6f8-4ae8-9554-ea89c061a014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, module in trainer.model.named_modules():\n",
    "#     if \"norm\" in name:\n",
    "#         module = module.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vXKsOC-Q_bp6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1873ac63-ee5f-4285-b96d-f286a5baf91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use callbacks (https://huggingface.co/docs/trl/sft_trainer#trl.SFTTrainer.callbacks) & https://huggingface.co/docs/transformers/v4.33.0/en/main_classes/callback#transformers.integrations.MLflowCallback\n",
    "\n",
    "with mlflow.start_run():\n",
    "  trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200f143f-c62f-4c1a-a1bc-c96d8df8351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(lora_adapter, save_adapter=True, save_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb76c575-88ea-4e81-93bb-91351b559b59",
   "metadata": {
    "id": "fb76c575-88ea-4e81-93bb-91351b559b59"
   },
   "source": [
    "### Merge model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034ab09e-a332-4bae-a34a-626c59953088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from peft import PeftModel\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer  # noqa: F402\n",
    "\n",
    "## Model\n",
    "model_id = \"meta-llama/Llama-2-13b-hf\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# When loading 1st time this will be slow\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    load_in_8bit=False, # Load in full precision\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\":0},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a778b1-1d9e-4feb-b752-faf794179f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    \"out/checkpoint-400\",\n",
    "    device_map={\"\":0},\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27371bb7-da90-4ba9-a608-14d9735889b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = lora_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c5f46b-255f-4b9f-9ca7-a5facc5ff464",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834eefb8-ffe2-48d7-bd05-2fdd2921ea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model.save_pretrained(\"merged/llama2-13B-instruct-dolly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fbd3dc-c122-4919-a891-2dc63fcbcbf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
