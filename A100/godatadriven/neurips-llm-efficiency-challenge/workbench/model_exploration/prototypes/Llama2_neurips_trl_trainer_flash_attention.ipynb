{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeee9895-8770-4dbd-a16a-a7f4a2fe40b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes 'optimum==1.13.1'\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q -U git+https://github.com/huggingface/trl@flash-attn-sft # to enable flash attention\n",
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da30f57e-6517-4930-bf95-57f088021530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://huggingface.co/docs/transformers/perf_train_gpu_one for efficient training tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccc2b77-eb4d-407c-b592-a6b9a8345416",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --index-url https://download.pytorch.org/whl/nightly/cu118 --pre 'torch>=2.1.0dev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb01fea-3b6c-4ffb-99f6-b0be0ff0a5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset , Dataset, concatenate_datasets \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "rd_ds = load_dataset(\"databricks/databricks-dolly-15k\")\n",
    "rd_df = pd.DataFrame(rd_ds['train'])\n",
    "display(rd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8320afee-a45d-46b1-a70f-2ceddfce34fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(example):\n",
    "    \"\"\"Generates a standardized message to prompt the model with an instruction, optional input and a\n",
    "    'response' field.\"\"\"\n",
    "\n",
    "    if example[\"context\"]:\n",
    "        return (\n",
    "            \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            f\"### Instruction:\\n{example['instruction']}\\n\\n### Input:\\n{example['context']}\\n\\n### Response:\"\n",
    "        )\n",
    "    return (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\"\n",
    "    )\n",
    "\n",
    "\n",
    "rd_df[\"prompt\"] = rd_df.apply(generate_prompt, axis=1)\n",
    "\n",
    "rd_df[\"response\"] = rd_df[\"response\"] + \"\\n### End\"\n",
    "\n",
    "rd_df = rd_df[[\"prompt\", \"response\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173d097c-b963-4a61-be33-c79e7609d38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "import os\n",
    "from transformers import BitsAndBytesConfig\n",
    "from trl import SFTTrainer\n",
    "import mlflow\n",
    "\n",
    "df = rd_df.copy()\n",
    "df[\"text\"] = df[\"prompt\"] + df[\"response\"]\n",
    "df.drop(columns=[\"prompt\", \"response\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e726cf-334d-4e3e-929e-d84a343285cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "dataset = Dataset.from_pandas(df).train_test_split(test_size=0.05, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdbf892-bc53-4028-80a2-e61b58af274b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_modules = ['gate_proj','down_proj','up_proj'] # , 'k_proj', 'lm_head', 'q_proj', 'v_proj', 'o_proj'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c13483e-69db-4325-aca5-bc7d72cef325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-13b-hf\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97a5f98-0d34-4159-ac32-1db2615f20a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d940b74-0c02-46d3-83db-ef9e9bc9f2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,#or r=16\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules = target_modules,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Done in sfttrainer\n",
    "#model = get_peft_model(model, lora_config)\n",
    "#model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e113e523-ec5b-462c-bd39-a0eb5aa98b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"out\"\n",
    "\n",
    "per_device_train_batch_size = 4\n",
    "gradient_accumulation_steps = 8 # virtual batch size = 4 * 8 = 32\n",
    "optim = 'paged_adamw_8bit'\n",
    "learning_rate = 4e-4\n",
    "max_grad_norm = 0.3\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = \"cosine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cae66a2-bc77-4a09-bfef-b1c32835618f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with 14260 examples, we get approx 446 steps for each epoch:\n",
    "14260 / 8 / 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f373486-8420-45f5-84eb-59738eb1dc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# https://huggingface.co/docs/transformers/v4.33.0/en/main_classes/trainer#transformers.TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=base_dir,\n",
    "    #save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    num_train_epochs = 1,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    #max_steps=100,\n",
    "    per_device_eval_batch_size=2,\n",
    "    eval_accumulation_steps=8,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    learning_rate=learning_rate,\n",
    "    bf16=True,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    warmup_steps=100,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    group_by_length=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b0406b-3074-4571-97c7-2440cb5f1b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=2048,\n",
    "    args=training_args,\n",
    "    use_flash_attn=True,\n",
    "    packing=True,\n",
    "    peft_config=lora_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ec7d24-f6f8-4ae8-9554-ea89c061a014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, module in trainer.model.named_modules():\n",
    "#     if \"norm\" in name:\n",
    "#         module = module.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2f545d-462b-488a-ba57-c79b32052de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1873ac63-ee5f-4285-b96d-f286a5baf91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c935e81-5049-428e-95f2-f5f7bbc3d791",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
