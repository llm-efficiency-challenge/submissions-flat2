{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cajajejo.training.trainer import NeuripsTrainer\n",
    "from cajajejo.training.utils import generate_prompt\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"cajajejo\")\n",
    "handler = logging.StreamHandler()\n",
    "format = logging.Formatter(\"%(name)s - %(levelname)s - %(message)s\")\n",
    "handler.setFormatter(format)\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "df_dolly = load_dataset(\"databricks/databricks-dolly-15k\")\n",
    "df_dolly = pd.DataFrame(df_dolly['train'])\n",
    "df_dolly\n",
    "\n",
    "df_dolly[\"prompt\"] = df_dolly.apply(generate_prompt, axis=1)\n",
    "df_dolly[\"response\"] = df_dolly[\"response\"] + \"\\n### End\"\n",
    "df_dolly = df_dolly[[\"prompt\", \"response\"]]\n",
    "\n",
    "df = df_dolly.copy()\n",
    "df[\"text\"] = df[\"prompt\"] + df[\"response\"]\n",
    "df.drop(columns=[\"prompt\", \"response\"], inplace=True)\n",
    "\n",
    "dataset = Dataset.from_pandas(df).train_test_split(test_size=0.05, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = '/home/user/neurips-llm-efficiency-challenge/jobs/training/opt1b_T4.job_config.yml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = NeuripsTrainer.from_config(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.get_model(mode=\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train_model(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    dataset_text_field=\"text\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = trainer.get_trained_lora_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = trainer.merge_lora_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.save_pretrained(\"out/merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = trainer.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path='/home/user/neurips-llm-efficiency-challenge/workbench/python_tool_experiments/out/merged', load_in_8bit=True, device_map={\"\": 0}, torch_dtype=torch.float16, local_files_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cajajejo.training.utils import extract_response_text, generate_prompt\n",
    "\n",
    "q = {\"instruction\": \"Give me a step-by-step explanation how I should make pizza\", \"context\": \"\"}\n",
    "\n",
    "#q={'instruction': 'What is the origin of orange wine?', \"context\":\"\"}\n",
    "\n",
    "prompt = generate_prompt(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_output = model.generate(\n",
    "  input_ids=input_ids, max_new_tokens=256\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(generation_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extract_response_text(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extract_response_text(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del peft_model\n",
    "del input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
