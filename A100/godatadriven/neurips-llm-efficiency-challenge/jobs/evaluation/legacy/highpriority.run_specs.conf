# Main `RunSpec`s for the benchmarking.
# NB: authors only use scenarios with priority 1 or 2
# Run specs below are the ones that were used for all models in the benchmark

# TODO: exchange your model for 'model=text' below

entries: [
  ##### Generic #####

  ##### Question Answering #####
  # Scenarios: BoolQ, NarrativeQA, NewsQA, QuAC
  # Scenarios: NaturalQuestions
  # Scenarios: CommonsenseQA, HellaSwag, OpenBookQA, TruthfulQA
  # Scenarios: MMLU

  ## Reading comprehension

  {description: "boolq:model=text,data_augmentation=canonical", priority: 1}
  {description: "narrative_qa:model=text,data_augmentation=canonical", priority: 2}
  {description: "quac:model=text,data_augmentation=canonical", priority: 1}

  ## Reading comprehension and closedbook QA variants

  {description: "natural_qa:model=text,mode=openbook_longans,data_augmentation=canonical", priority: 1}
  {description: "natural_qa:model=text,mode=closedbook,data_augmentation=canonical", priority: 1}

  ## Closed-book QA with multiple choice

  # Adaptation method is set to ADAPT_MULTIPLE_CHOICE_SEPARATE_CALIBRATED and echo=True
  {description: "commonsense:model=full_functionality_text,dataset=hellaswag,method=multiple_choice_separate_original,data_augmentation=canonical", priority: 1}
  {description: "commonsense:model=full_functionality_text,dataset=openbookqa,method=multiple_choice_separate_calibrated,data_augmentation=canonical", priority: 2}
  {description: "truthful_qa:model=text,task=mc_single,data_augmentation=canonical", priority: 1}

  {description: "mmlu:model=text,subject=abstract_algebra,data_augmentation=canonical", priority: 2}
  {description: "mmlu:model=text,subject=college_chemistry,data_augmentation=canonical", priority: 2}
  {description: "mmlu:model=text,subject=computer_security,data_augmentation=canonical", priority: 2}
  {description: "mmlu:model=text,subject=econometrics,data_augmentation=canonical", priority: 2}
  {description: "mmlu:model=text,subject=us_foreign_policy,data_augmentation=canonical", priority: 2}

  ##### Information Retrieval #####
  # Scenarios: MS Marco (Regular), MS MARCO (TREC)

  {description: "msmarco:model=full_functionality_text,data_augmentation=canonical,track=regular,valid_topk=30", priority: 2}
  {description: "msmarco:model=full_functionality_text,data_augmentation=canonical,track=trec,valid_topk=30", priority: 1}

  ##### Summarization #####
  # Scenarios: XSUM, CNN/DM

  {description: "summarization_cnndm:model=text,temperature=0.3,device=cpu", priority: 1}
  {description: "summarization_xsum_sampled:model=text,temperature=0.3,device=cpu", priority: 1}


  ##### Sentiment Analysis #####
  # Scenarios: IMDB

  {description: "imdb:model=text,data_augmentation=canonical", priority: 1}


  ##### (Miscellaneous) Text Classification #####
  # Scenarios: RAFT

  {description: "raft:subset=ade_corpus_v2,model=text,data_augmentation=canonical", priority: 2}
  {description: "raft:subset=banking_77,model=text,data_augmentation=canonical", priority: 2}
  {description: "raft:subset=neurips_impact_statement_risks,model=text,data_augmentation=canonical", priority: 2}
  {description: "raft:subset=one_stop_english,model=text,data_augmentation=canonical", priority: 2}
  {description: "raft:subset=overruling,model=text,data_augmentation=canonical", priority: 2}
  {description: "raft:subset=semiconductor_org_types,model=text,data_augmentation=canonical", priority: 2}
  {description: "raft:subset=tweet_eval_hate,model=text,data_augmentation=canonical", priority: 2}
  {description: "raft:subset=twitter_complaints,model=text,data_augmentation=canonical", priority: 2}
  {description: "raft:subset=systematic_review_inclusion,model=text,data_augmentation=canonical", priority: 2}
  {description: "raft:subset=tai_safety_research,model=text,data_augmentation=canonical", priority: 2}
  {description: "raft:subset=terms_of_service,model=text,data_augmentation=canonical", priority: 2}


  ##### Toxicity Detection #####
  # Scenarios: CivilComments

  {description: "civil_comments:model=text,demographic=all,data_augmentation=canonical", priority: 1}
  {description: "civil_comments:model=text,demographic=male,data_augmentation=canonical", priority: 2}
  {description: "civil_comments:model=text,demographic=female,data_augmentation=canonical", priority: 2}
  {description: "civil_comments:model=text,demographic=LGBTQ,data_augmentation=canonical", priority: 2}
  {description: "civil_comments:model=text,demographic=christian,data_augmentation=canonical", priority: 2}
  {description: "civil_comments:model=text,demographic=muslim,data_augmentation=canonical", priority: 2}
  {description: "civil_comments:model=text,demographic=other_religions,data_augmentation=canonical", priority: 2}
  {description: "civil_comments:model=text,demographic=black,data_augmentation=canonical", priority: 2}
  {description: "civil_comments:model=text,demographic=white,data_augmentation=canonical", priority: 2}

  ##### TARGETED EVALUATIONS (p.37 in paper)

  ##### Component Skills and Risks #####

  ##### Language #####
  # Scenarios: BLiMP, The Pile, ICE, WikiText-103, TwitterAAE

  # We select 4 phenomena to elevate to priority 2, one per linguistic field.
  # The phenomena in BLiMP are annotated belong to one of the following 4 linguistic fields:
  # Morphology, Semantics, Syntax, and Syntax-Semantics
  # Beyond ensuring coverage of these 4 fields, to choose the higher priority representative,
  # we choose the phenomena within the field with the lowest GPT-2 performance reported (Warsadt et al., 2020).
  {description: "blimp:model=full_functionality_text,phenomenon=irregular_forms", priority: 2} # Morphology
  {description: "blimp:model=full_functionality_text,phenomenon=quantifiers", priority: 2} # Semantics
  {description: "blimp:model=full_functionality_text,phenomenon=island_effects", priority: 2} # Syntax
  {description: "blimp:model=full_functionality_text,phenomenon=binding", priority: 2} # Syntax-Semantics

  ## Language modeling
  {description: "the_pile:model=full_functionality_text,subset=ArXiv", priority: 2}
  {description: "the_pile:model=full_functionality_text,subset=BookCorpus2", priority: 2}
  {description: "the_pile:model=full_functionality_text,subset=Enron Emails", priority: 2}
  {description: "the_pile:model=full_functionality_text,subset=Github", priority: 2}
  {description: "the_pile:model=full_functionality_text,subset=PubMed Central", priority: 2}
  {description: "the_pile:model=full_functionality_text,subset=Wikipedia (en)", priority: 2}

  {description: "twitter_aae:model=full_functionality_text,demographic=aa", priority: 1}
  {description: "twitter_aae:model=full_functionality_text,demographic=white", priority: 1}

  {description: "ice:model=full_functionality_text,subset=ea", priority: 2}
  {description: "ice:model=full_functionality_text,subset=hk", priority: 2}
  {description: "ice:model=full_functionality_text,subset=ind", priority: 2}

  {description: "ice:model=full_functionality_text,gender=female", priority: 2}
  {description: "ice:model=full_functionality_text,gender=male", priority: 2}

  ##### Knowledge #####

  # For WikiFact, we sampled the following 10 relation types, which cover diverse topics
  # across general facts, humanities, social sciences and STEM.
  {description: "wikifact:model=text,k=5,subject=plaintiff", priority: 2}
  {description: "wikifact:model=text,k=5,subject=place_of_birth", priority: 2}
  {description: "wikifact:model=text,k=5,subject=medical_condition_treated", priority: 2}
  {description: "wikifact:model=text,k=5,subject=instance_of", priority: 2}
  {description: "wikifact:model=text,k=5,subject=part_of", priority: 2}
  {description: "wikifact:model=text,k=5,subject=currency", priority: 2}
  {description: "wikifact:model=text,k=5,subject=position_held", priority: 2}
  {description: "wikifact:model=text,k=5,subject=author", priority: 2}
  {description: "wikifact:model=text,k=5,subject=discoverer_or_inventor", priority: 2}
  {description: "wikifact:model=text,k=5,subject=symptoms_and_signs", priority: 2}

  ##### Reasoning #####

  # Code models outperform text models on reasoning tasks.
  # Evaluate all language models (model=text_code) on reasoning scenarios.

  ## Synthetic
  # TODO: had to disable these due to a refactor
  # {description: "numeracy:model=text_code,run_solver=True,relation_type=linear,mode=function", priority: 2}
  # {description: "numeracy:model=text_code,run_solver=True,relation_type=plane,mode=function", priority: 3}

  # The DistanceMetric is slow to compute for relation_type 'parabola' and 'paraboloid', so set run_solver=False
  # {description: "numeracy:model=text_code,run_solver=False,relation_type=parabola,mode=function", priority: 4}
  # {description: "numeracy:model=text_code,run_solver=False,relation_type=paraboloid,mode=function", priority: 4}

  {description: "synthetic_reasoning:model=text_code,mode=pattern_match", priority: 2}
  {description: "synthetic_reasoning:model=text_code,mode=variable_substitution", priority: 2}
  {description: "synthetic_reasoning:model=text_code,mode=induction", priority: 2}

  {description: "synthetic_reasoning_natural:model=text_code,difficulty=easy", priority: 2}
  {description: "synthetic_reasoning_natural:model=text_code,difficulty=hard", priority: 2}

  {description: "babi_qa:model=text_code,task=all", priority: 2}
  {description: "babi_qa:model=text_code,task=3", priority: 2}
  {description: "babi_qa:model=text_code,task=15", priority: 2}
  {description: "babi_qa:model=text_code,task=19", priority: 2}

  {description: "dyck_language:model=text_code,num_parenthesis_pairs=3", priority: 2}

  ## Real

  {description: "math:model=text_code,subject=number_theory,level=1,use_official_examples=True", priority: 2}
  {description: "math:model=text_code,subject=intermediate_algebra,level=1,use_official_examples=True", priority: 2}
  {description: "math:model=text_code,subject=algebra,level=1,use_official_examples=True", priority: 2}
  {description: "math:model=text_code,subject=prealgebra,level=1,use_official_examples=True", priority: 2}
  {description: "math:model=text_code,subject=geometry,level=1,use_official_examples=True", priority: 2}
  {description: "math:model=text_code,subject=counting_and_probability,level=1,use_official_examples=True", priority: 2}
  {description: "math:model=text_code,subject=precalculus,level=1,use_official_examples=True", priority: 2}

  # With chain-of-thought prompting:
  {description: "math:model=text_code,subject=number_theory,level=1,use_chain_of_thought=True", priority: 2}
  {description: "math:model=text_code,subject=intermediate_algebra,level=1,use_chain_of_thought=True", priority: 2}
  {description: "math:model=text_code,subject=algebra,level=1,use_chain_of_thought=True", priority: 2}
  {description: "math:model=text_code,subject=prealgebra,level=1,use_chain_of_thought=True", priority: 2}
  {description: "math:model=text_code,subject=geometry,level=1,use_chain_of_thought=True", priority: 2}
  {description: "math:model=text_code,subject=counting_and_probability,level=1,use_chain_of_thought=True", priority: 2}
  {description: "math:model=text_code,subject=precalculus,level=1,use_chain_of_thought=True", priority: 2}

  {description: "gsm:model=text_code", priority: 2}

  # Legal reasoning
  {description: "legal_support:model=text_code", priority: 2}

  {description: "lsat_qa:model=text_code,task=all", priority: 2}

  # MedQA
  {description: "med_qa:model=biomedical", priority: 2}

  # Data processing

  {description: "entity_matching:model=text,dataset=Beer", priority: 2}
  {description: "entity_matching:model=text,dataset=Abt_Buy", priority: 2}
  {description: "entity_matching:model=text,dataset=Dirty_iTunes_Amazon", priority: 2}

  {description: "entity_data_imputation:model=text,dataset=Buy", priority: 2}
  {description: "entity_data_imputation:model=text,dataset=Restaurant", priority: 2}

  # Code
  {description: "code:model=code,dataset=humaneval", priority: 1}
  {description: "code:model=code,dataset=apps,timeout=3", priority: 1}

  ##### Harms #####

  ## Copyright

  # Randomly sampled instances from the original BooksCorpus.
  # We expect data here to be less repeated in the pretraining corpus. This approximates the average case.
  {description: "copyright:model=text,datatag=n_books_1000-extractions_per_book_1-prefix_length_125", priority: 1}

  # We expect data here to be repeated more in the pretraining corpus. This approximates the worst case.
  {description: "copyright:model=text,datatag=popular_books-prefix_length_125.json", priority: 1}

  # Large and small codex models.
  {description: "copyright:model=code,datatag=prompt_num_line_1-min_lines_20.json", priority: 2}
  {description: "copyright:model=code,datatag=prompt_num_line_10-min_lines_20.json", priority: 2}

  ## Disinformation

  {description: "disinformation:model=text,capability=reiteration,topic=climate", priority: 1}
  {description: "disinformation:model=text,capability=reiteration,topic=covid", priority: 1}
  {description: "disinformation:model=text,capability=wedging", priority: 1}

  ## Bias

  {description: "bbq:model=text,subject=all", priority: 2}

  ## Toxicity

  {description: "real_toxicity_prompts:model=text", priority: 2}

  {description: "bold:model=text,subject=all", priority: 2}


  ##### Efficiency #####

  {description: "synthetic_efficiency:model=text,tokenizer=default,num_prompt_tokens=default_sweep,num_output_tokens=default_sweep", priority: 1}
  {description: "synthetic_efficiency:model=code,tokenizer=default,num_prompt_tokens=default_sweep,num_output_tokens=default_sweep", priority: 1}

  ##### Robustness #####

  ## Contrast sets (these are separate runs since we will only consider Instances that have contrast sets
  {description: "boolq:model=text,only_contrast=True,data_augmentation=contrast_sets", priority: 2, groups: ["robustness_contrast_sets"]}
  {description: "imdb:model=text,only_contrast=True,data_augmentation=contrast_sets", priority: 2, groups: ["robustness_contrast_sets"]}

  ##### Instruction Following #####

  {description: "self_instruct:model=instruction_following,num_respondents=1", priority: 1}
  {description: "grammar:path=src/helm/benchmark/scenarios/best_chatgpt_prompts.yaml,tags=,model=instruction_following,num_respondents=1", priority: 1}
  {description: "open_assistant:language=en,model=instruction_following,num_respondents=1", priority: 1}
  {description: "vicuna:model=instruction_following,num_respondents=1", priority: 1}
  {description: "koala:model=instruction_following,num_respondents=1", priority: 1}
  {description: "anthropic_hh_rlhf:subset=hh,model=instruction_following,num_respondents=1", priority: 1}
]
