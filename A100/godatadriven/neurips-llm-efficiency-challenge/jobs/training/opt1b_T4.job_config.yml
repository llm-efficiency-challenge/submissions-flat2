model: facebook/opt-1.3b
version: v1
sft_trainer_config:
  max_seq_length: 2048
bits_and_bytes_config:
  load_in_4bit: true
  bnb_4bit_config:
    bnb_4bit_use_double_quant: true
    bnb_4bit_quant_type: nf4
    bnb_4bit_compute_dtype: bfloat16
lora_config:
  target_modules:
    - k_proj
    - v_proj
    - q_proj
  r: 16
  lora_alpha: 16
  lora_dropout: 0.05
  bias: none
  task_type: CAUSAL_LM
tokenizer_config:
  use_fast: true
train_arguments_config:
  output_dir: out
  save_strategy: steps
  save_steps: 100
  save_total_limit: 3
  num_train_epochs: 1
  evaluation_strategy: steps
  logging_strategy: steps
  eval_steps: 100
  logging_steps: 25
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 8
  gradient_checkpointing: true
  per_device_eval_batch_size: 2
  eval_accumulation_steps: 8
  learning_rate: 0.0004
  lr_scheduler_type: cosine
  max_grad_norm: 0.3
  warmup_steps: 100
  optim: paged_adamw_8bit
  dataloader_pin_memory: true
  dataloader_num_workers: 4
  fp16: true
  group_by_length: true
  torch_compile: false
model_merger_config:
  load_in_8bit: false
  lora_checkpoint: latest
  torch_dtype: float16
mlflow_config:
  tracking_uri: http://10.164.0.15:5000
tracking_config:
  experiment_name: train-debug
  # See https://yaml-multiline.info/. Only '>' is supported here.
  description: >
    This training run finetunes a Llama2 13B base model on dolly-15k.
  tags:
    model: LLama2-13B
    creator: Jasper
    lifecycle: experimental
