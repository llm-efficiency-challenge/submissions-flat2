# llm evaluation

## Getting Started

### 1. llm-efficiency-challenge

#### Databricks-Dolly-15

databricks-dolly-15k is an open source dataset of instruction-following records generated by thousands of Databricks employees in several of the behavioral categories outlined in the InstructGPT paper, including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization.<br>
[Hugging Face Link](https://huggingface.co/datasets/databricks/databricks-dolly-15k)

#### OpenAssistant Conversations Dataset (oasst1)

penAssistant Conversations (OASST1), a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 quality ratings, resulting in over 10,000 fully annotated conversation trees. The corpus is a product of a worldwide crowd-sourcing effort involving over 13,500 volunteers.<br>
[Hugging Face Link](https://huggingface.co/datasets/OpenAssistant/oasst1)

#### The Flan Collection

The Flan Collection compiles datasets from Flan 2021, P3, Super-Natural Instructions, along with dozens more datasets into one place, formats them into a mix of zero-shot, few-shot and chain-of-thought templates, then mixes these in proportions that are found to achieve strong results on held-out evaluation benchmarks, as reported for Flan-T5 and Flan-PaLM in the Scaling Flan paper and Flan Collection paper.
[Github](https://github.com/google-research/FLAN/tree/main/flan/v2)

#### Allenai Dolma

3 trillion tokens from a diverse mix of web content, academic publications, code, books, and encyclopedic materials. It is openly released under AI2â€™s ImpACT license as a medium risk artifact.
[Hugging Face Link](https://huggingface.co/datasets/allenai/dolma)

#### RedPajama-Data-1T

RedPajama is a clean-room, fully open-source implementation of the LLaMa dataset.
<table border="1">
  <tr>
    <th>Dataset</th>
    <th>Token Count</th>
  </tr>
  <tr>
    <td>Commoncrawl</td>
    <td>878 Billion</td>
  </tr>
  <tr>
    <td>C4</td>
    <td>175 Billion</td>
  </tr>
  <tr>
    <td>GitHub</td>
    <td>59 Billion</td>
  </tr>
  <tr>
    <td>Books</td>
    <td>26 Billion</td>
  </tr>
  <tr>
    <td>ArXiv</td>
    <td>28 Billion</td>
  </tr>
  <tr>
    <td>Wikipedia</td>
    <td>24 Billion</td>
  </tr>
  <tr>
    <td>StackExchange</td>
    <td>20 Billion</td>
  </tr>
  <tr>
    <td>Total</td>
    <td>1.2 Trillion</td>
  </tr>
  <!-- Add more rows as needed -->
</table>

[Hugging Face Link](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T)

#### OPENAI's step by step for math?





### 2. other dataset

### TODO:
1. Dolly? databricks/databricks-dolly-15k

### NOTES:
1. 
