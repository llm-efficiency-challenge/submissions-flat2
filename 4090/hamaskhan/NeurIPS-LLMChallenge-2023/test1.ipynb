{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cca1175-95a4-42c7-9a36-4558008d3162",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d006754-b775-4d00-99d3-83cae915a768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e1ef1f-259d-44d5-84c6-84b678fad53c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker build -f ./Dockerfile.train -t llama_recipes_train ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d31903-598f-4bfc-bb0d-3d34726ab847",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker run --gpus \"device=0\" --rm -ti llama_recipes_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "730a3128-7ae8-498c-a76d-1e10c36c2622",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  169.5kB\n",
      "Step 1/11 : FROM pytorch/pytorch:2.1.0-cuda12.1-cudnn8-devel\n",
      " ---> 6f873147bf83\n",
      "Step 2/11 : RUN apt-get update  && apt-get install -y git python3-virtualenv wget vim\n",
      " ---> Using cache\n",
      " ---> 033ca66a1108\n",
      "Step 3/11 : RUN pip install -U --no-cache-dir git+https://github.com/facebookresearch/llama-recipes.git@eafea7b366bde9dc3f0b66a4cb0a8788f560c793\n",
      " ---> Using cache\n",
      " ---> 4e1b8cffdf09\n",
      "Step 4/11 : WORKDIR /workspace\n",
      " ---> Using cache\n",
      " ---> 8f9f0b1302bb\n",
      "Step 5/11 : COPY ./fast_api_requirements.txt fast_api_requirements.txt\n",
      " ---> 9987ff91dded\n",
      "Step 6/11 : RUN pip install --no-cache-dir --upgrade -r fast_api_requirements.txt\n",
      " ---> Running in 8620c82191c4\n",
      "Collecting fastapi<0.69.0,>=0.68.0 (from -r fast_api_requirements.txt (line 2))\n",
      "  Downloading fastapi-0.68.2-py3-none-any.whl (52 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.4/52.4 kB 4.6 MB/s eta 0:00:00\n",
      "Collecting pydantic<2.0.0,>=1.8.0 (from -r fast_api_requirements.txt (line 3))\n",
      "  Obtaining dependency information for pydantic<2.0.0,>=1.8.0 from https://files.pythonhosted.org/packages/e0/2f/d6f17f8385d718233bcae893d27525443d41201c938b68a4af3d591a33e4/pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (149 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.6/149.6 kB 12.2 MB/s eta 0:00:00\n",
      "Collecting uvicorn<0.16.0,>=0.15.0 (from -r fast_api_requirements.txt (line 4))\n",
      "  Downloading uvicorn-0.15.0-py3-none-any.whl (54 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.4/54.4 kB 24.0 MB/s eta 0:00:00\n",
      "Collecting starlette==0.14.2 (from fastapi<0.69.0,>=0.68.0->-r fast_api_requirements.txt (line 2))\n",
      "  Downloading starlette-0.14.2-py3-none-any.whl (60 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.6/60.6 kB 21.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<2.0.0,>=1.8.0->-r fast_api_requirements.txt (line 3)) (4.7.1)\n",
      "Collecting asgiref>=3.4.0 (from uvicorn<0.16.0,>=0.15.0->-r fast_api_requirements.txt (line 4))\n",
      "  Obtaining dependency information for asgiref>=3.4.0 from https://files.pythonhosted.org/packages/9b/80/b9051a4a07ad231558fcd8ffc89232711b4e618c15cb7a392a17384bbeef/asgiref-3.7.2-py3-none-any.whl.metadata\n",
      "  Downloading asgiref-3.7.2-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn<0.16.0,>=0.15.0->-r fast_api_requirements.txt (line 4)) (8.0.4)\n",
      "Collecting h11>=0.8 (from uvicorn<0.16.0,>=0.15.0->-r fast_api_requirements.txt (line 4))\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 34.3 MB/s eta 0:00:00\n",
      "Downloading pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 19.9 MB/s eta 0:00:00\n",
      "Downloading asgiref-3.7.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: starlette, pydantic, h11, asgiref, uvicorn, fastapi\n",
      "Successfully installed asgiref-3.7.2 fastapi-0.68.2 h11-0.14.0 pydantic-1.10.13 starlette-0.14.2 uvicorn-0.15.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 8620c82191c4\n",
      " ---> 496394ab224b\n",
      "Step 7/11 : ENV HUGGINGFACE_TOKEN=\"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\"\n",
      " ---> Running in 841eed3ee46e\n",
      "Removing intermediate container 841eed3ee46e\n",
      " ---> 11c58997c7e8\n",
      "Step 8/11 : ENV HUGGINGFACE_REPO=\"mhkhan/neurips_llm\"\n",
      " ---> Running in cbc1c8eddc97\n",
      "Removing intermediate container cbc1c8eddc97\n",
      " ---> 3149eb4d3cdb\n",
      "Step 9/11 : COPY ./main.py main.py\n",
      " ---> fbae1c320f76\n",
      "Step 10/11 : COPY ./api.py api.py\n",
      " ---> fce2a11fe32c\n",
      "Step 11/11 : CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"80\"]\n",
      " ---> Running in 18428b56861e\n",
      "Removing intermediate container 18428b56861e\n",
      " ---> 77495e1cf8d1\n",
      "Successfully built 77495e1cf8d1\n",
      "Successfully tagged llama_recipes_inference:latest\n"
     ]
    }
   ],
   "source": [
    "!docker build -f ./Dockerfile -t llama_recipes_inference ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f3d2d8-277c-4b73-83be-90c7a8487f89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
